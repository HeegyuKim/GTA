{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\d-project\\detox\\GeDi\n"
     ]
    }
   ],
   "source": [
    "%cd GeDi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import jsonlines\n",
    "from tqdm.autonotebook  import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from modeling_gpt2 import GPT2LMHeadModel\n",
    "\n",
    "from transformers import (\n",
    "    GPT2Config,\n",
    "    GPT2Tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration SetFit--bbc-news-dbeb222bfdd4d6ed\n",
      "Found cached dataset json (C:/Users/heegyukim/.cache/huggingface/datasets/SetFit___json/SetFit--bbc-news-dbeb222bfdd4d6ed/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    }
   ],
   "source": [
    "data_name = \"SetFit/bbc-news\"\n",
    "dataset = load_dataset(data_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no logit scale initialized for gpt2\n"
     ]
    }
   ],
   "source": [
    "mode = \"detoxifier\"\n",
    "code_desired = \"dirty\"\n",
    "code_undesired = \"clean\"\n",
    "model_type = 'gpt2'\n",
    "gen_type = \"gedi\"\n",
    "gen_model_name_or_path = \"gpt2\" # \"gpt2-medium\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_CLASSES = {\"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),}\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[\"gpt2\"]\n",
    "tokenizer = tokenizer_class.from_pretrained(gen_model_name_or_path, do_lower_case=False)\n",
    "\n",
    "model = model_class.from_pretrained(gen_model_name_or_path)#, load_in_half_prec=True)\n",
    "model = model.to(device)\n",
    "\n",
    "gedi_model_name_or_path = 'gedi_detoxifier'\n",
    "gedi_model = model_class.from_pretrained(gedi_model_name_or_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting arguments for generation\n",
    "#max generation length\n",
    "gen_length = 200\n",
    "#omega from paper, higher disc_weight means more aggressive topic steering (30)\n",
    "disc_weight = 30\n",
    "#1 - rho from paper, should be between 0 and 1 higher filter_p means more aggressive topic steering\n",
    "filter_p = 0.8\n",
    "#tau from paper, preserves tokens that are classified as correct topic\n",
    "target_p = 0.8\n",
    "#hyperparameter that determines class prior, set to uniform by default\n",
    "class_bias = 0\n",
    "\n",
    "if gen_length>1024:\n",
    "  length = 1024\n",
    "else:\n",
    "  length = gen_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"only Boulder as a nuclear power reactor which comes to mind,\\n\\nthis shows that people don't understand it at all. When I was far away between Alaska and Idaho my assignment centers were sunny on sunshine days (and back roads not). Living there grew up in suburban homes; if you weren't able to rouse or vent your fears about pollution, the community would have shrapnel dropped thousands of feet from shores like 70 stories of guttering wood used by sailors for vibrating hair removal and grease treatment. Just one friend who had similar problems overheard his parents digging dirt out an embankment when he got out; he forgot their attendance laxity for ever filling these private spaces with heavy weeds once the place looked more warm from outside across a 41 mile stretch. Eventually 71 was available—only 40 miles east of Boulder as pictured above, but standing 200 yards apart along with 14 clams—with Rangers together guiding the gleaming powder hole through field after field in this way showcasing its astonishing potential to change people's lives---kept us kyled down my potholes here. Evening NYC week after week sinceurming by Anaheim locals along505 Piedmont Avenue completely remained largely unchanged besides occasional bloating in late 1974 and early 1975 has broken every conservation plan into eight squares that each meet similar perimeters (Notice sludge meets shape), our land use patterns date back cave period China 1/3–37–nexcient -base plat but other places still take advantage! One famous chainpitch protected stripbuilt low elevation plover earlier atJuicyka1 Southon slices bulk so slowly chinado-shining crud dirtied ankle fabric shaped resin contains six times higher stamens volatile antiparasitic because acid element 118 wipes reliably doesn't mean spelling 'burMist.'71 revsystem Greater Chicago east climbing system joint failure deadliest hazard Alpha brewery-shaped junkyard crosslinking extreme weather ATMACS Advanced Electrician wanting further evidence DLN ForkFacility Mid\",\n",
       " 'neat graphics. Especially added an option to show background image and pie charts as part of FPSCorrection Action Center and a new view with a better score, blackTop Horizontal Horizontal aspect ratio, pitch speed indicators (no AI ping per gif), cropping topics [GuideAll] \"FastFPSFigure\"2: cosine performed by @alubran_island while boomie comprised all these options. Special thanks to #x3irulmon for the creation of his architecture\\n\\nlink://argphoproductsaurnujpalhkhvir65afternoon An important space now used for accessibility & MtGPAGood white scale gamesTech game tech Goldfish avatar Mechagori all different ability spellsMonster bar - Full weather tiles appear needed too Seems slow~Online spending time skyfilled stormless Lands away we go! Upgrade yo what language vocabending emergent wrestling Players Confining this chemist skype activitiesFocus on management%all for SeaRoaming What service down partition I am having trouble figuring out!!!? Resolution is limited?Ski remove IM response from yumBosshe supported curated purchasing / hostsSpace services community domain active O vigil un nation vy Grays afro 8 networks have modsout dailycoin weight 3 lux biting retarded capsule http:// goingofuriatic FOMO Why would you trust such little trader??? \\'+) Once USSR puppet cafe Redux Yup everybody went bankruptCan\\'t build right railings ion phases helper theme sucked Voice chat + plug support not useabacking CRICKET Editusers crazy spam till midnightHa…?! Caniyo lets keep positioning ya r boundaries flarpost needs recovery guy LOVEPAKS!!!!!!!! Pumped insert Has Should we do shinning purple nethersteels White Spec raid boss afterclosotaur baby stepead cowboy kazid reveller606 xii:1 compilation logo that wasn\\'t on googledo loadingunknown foryssMenu most days szeever Meaning such overwhelming body mass production',\n",
       " \"only factory sealed and not champagne shots, _ because it has so many additives to make the bottles effective! It handles butter completely well_. Today I am definitely going to do a Cleanup SP campaign. A lot of people are either Harry Chapaliki or Mark Henry. So Jack sells me one(http://ldchukinthecoastofterrierkingsshido!!!...\\nGod bless Ryan J!! Such insanely simple kitsstodown shakers, yessss avidon itself i have been needed for over 5 years now when yungre looked rubbing wonderful brands from bit overboard with patrimonies ever had and'm sold like crazy growing up in matrix in the prog for almost 2 decades before family borrowing stopped all auctioning its now there was thank you family great forum fursuit Mark/DJ Webs raveies,i yet again plant coment all moved into my srs company ambron 37e malware hacking hotdown 01/27 50 ISO sealati They stroll living alone most days at eachfalls intersection linden yrry birthday how on earth CNB said chime yesterday he across lea, which instantly blinded him moved ax after 24 hours 45 wittle weather 8 data levels thought Keystone closure lastuz grounded did x epidermis avsin 31-09-01 Strophie turn sun impressed floor guy body leak by and anti respiratory vet 13 buzzer ash tunnelrs is ages since smart friends reported your piece nice coopissues so blitz11yz latest dusty workcity slush green seven most random trove failure holoused hungra patsy statement They're doing a merchant run's then sit id backing out flo currencies slushers stankitect munflirteen two od minimum assistants iris driven northwest. modifying fabers ur versions expired helm fresh barg lived bigger with Mike Witman skalt oils bilarfow Dan echoes still person saw through hues excited oy 50 pound run watersz overturn approach pawn palmetto ivacc sa gigg climbing altitude tab :\",\n",
       " \"worth keeping\\n\\nin mind that it could be strictly suggested, with this in mind, to allow Wii U controller input for ever-using singleplayer too!\\n\\n\\nSimply switch over into Switch and turn your controller on again! How easy: Every run without the knowledge needed to complete extra sections isn't enough. Jumping off a ledge broken or busted means they can actually grab on at least one of the vents from Super Mario World. Slowly neck hard (depending on how long you've plugged in) lowers recoil values and improving accuracy greatly makes stalking lower ground free pressing spots easier for most players around Luigi more technical than level ranges or terrain types where a good dependence is needed harder melee armadillo jumping deceleration increases shield penetration\\n\\nMore advice Upgrades: 'Confirm' aids manipulation who's no longer actively travelddark as quickly as when standing, but now must wait patiently before rebounding (up std): timeout other ally unlocks completion of interior option blink(3),flag 8 fails staying invulnerable while crouching into rooftop rooms; optional deliberation/clearscreen allows all levels room doors homing while not resting laze(5th),Flag 9 seems to remove half to straak location invisible lighting work remains real otoacer increased hip shot power play defenses advantage one ability cannot cover all adjacent entities These are fairly old updates only and should not now be replaced randomly though...some more touting survives worldwide aortion fading backrier battery must gently fill pack position automatics Will help make collecting soul gem events main screen admin doesn't show 3 messages clearly select saved audio taunt button may stay briefly enabled if alt turns asleep Otherwise next few trophies will get miscalculated sender localized sprightly tl story eape puzzler tasks up note fight transposability Destiny IV support fully multiplayer improvements Phantom Blur PSN localization announced Lightning Turbo once due soon unit type mission functionalitywork correctly fixed thinner popup button attacks were combo attempts use infinite items until kill unsafe theories for potentially circum\",\n",
       " 'only ever in crisis when there is communication with Hamburg. We don\\'t see far away and myself bean count probably 20-50 people left May 8th.\"\\n\\nJarachand said Russia wrote his letter because its alien flows will not reach Europe\\'s southern border. He said: \"If you cannot find anyone, the first target won\\'t be France; it will be Germany.\" High tensions coupled with a dumbass economy give input to resistance on Russia\\'s part alone, but if German right-leaning parties take power drastically sooner and instead ratchet down criticism of Putin, Austria would become a problem again. The insurers reorganised properly once they should have never been as vulnerable at bad risk as now? \"The only trade experience I\\'m able to think of that describes those two travel generators can talk me into helping them,\" Jarachand says about any notion trying soft forms Greece welcome were its de facto bank money will be asked now … soon after 1 August (unless Irma recedes). Quote Tajik news bulletins printed for one month before bankruptcy could even fit on metal borders that bind interior groups quickly…… – Former Austrian Custom 2014 attached pictures via Aim3MOS Paulo Alonso\\n\\n\\nContributing: Alexis Szkavalinka\\n\\n\\nThe Independent has launched its #FinalSay campaign to demand that voters are given a voice in the final Brexit deal.\\n\\n\\nSign our petition here\\n and we\\'ll send you three meaningful actions you can take each week. Thank you for signing the No 2020 pledge! Sign your comments below, along with the option of staying in your earlier post or continuing to share our popular campaigns like Proclaim Our Future without advertising or campaigning by cat Twitter Getty Images 15/42 Jeremy Corbyn London Leader Theresa May met Mr Corbyn during their phone call just days after announcing Britain was leaving the EU Getty Images 16/42 The Labour leader found himself pressed against a wall outside the headquarters of his local government while opposing an entirely Remain vote PA 17/42 Liberal Democrat leader Tim Far']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_length = 400\n",
    "def generate_text(prompt):\n",
    "  text_ids = tokenizer.encode(prompt)\n",
    "  encoded_prompts=torch.LongTensor(text_ids).unsqueeze(0).to(device)\n",
    "  if encoded_prompts.shape[1] > 512:\n",
    "    encoded_prompts = encoded_prompts[:, -512:]\n",
    "\n",
    "  # multi_code = tokenizer.encode(secondary_code)\n",
    "  attr_class = 1\n",
    "\n",
    "  generated_sequence = model.generate(\n",
    "    input_ids=encoded_prompts,\n",
    "    pad_lens=None,\n",
    "    max_length=min(1024, encoded_prompts.shape[1] + gen_length),\n",
    "    min_length=min(1024, encoded_prompts.shape[1] + gen_length),\n",
    "    top_k=None,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty= 1.2,\n",
    "    rep_penalty_scale= 10,\n",
    "    eos_token_ids = tokenizer.eos_token_id,\n",
    "    pad_token_id = 0,\n",
    "    do_sample= True,\n",
    "    penalize_cond= True,\n",
    "    gedi_model= gedi_model,\n",
    "    tokenizer= tokenizer,\n",
    "    disc_weight= disc_weight,\n",
    "    filter_p = filter_p,\n",
    "    target_p = target_p,\n",
    "    class_bias = class_bias,\n",
    "    attr_class = attr_class,\n",
    "    code_0 = code_desired,\n",
    "    code_1 = code_undesired,\n",
    "    multi_code=None,\n",
    "    num_return_sequences=5\n",
    "    )\n",
    "\n",
    "  texts = [tokenizer.decode(output, skip_special_tokens=True)[len(prompt):] for output in generated_sequence.tolist()[0]]\n",
    "  return texts\n",
    "\n",
    "prompt = \"It is really \"\n",
    "generate_text(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1225 [02:23<8:26:20, 24.92s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2987 > 1024). Running this sequence through the model will result in indexing errors\n",
      "  1%|          | 10/1225 [03:36<6:41:30, 19.83s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1051 > 1024). Running this sequence through the model will result in indexing errors\n",
      "  3%|▎         | 35/1225 [11:23<6:01:39, 18.23s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1104 > 1024). Running this sequence through the model will result in indexing errors\n",
      "  5%|▌         | 62/1225 [19:43<5:53:10, 18.22s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1996 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 10%|▉         | 117/1225 [36:28<5:43:44, 18.61s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1114 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 14%|█▍        | 175/1225 [52:52<4:56:45, 16.96s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1279 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 16%|█▌        | 196/1225 [59:05<5:12:32, 18.22s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (4461 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 17%|█▋        | 206/1225 [1:02:02<4:46:47, 16.89s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 20%|██        | 246/1225 [1:13:54<4:46:57, 17.59s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 22%|██▏       | 266/1225 [1:19:21<4:37:34, 17.37s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 25%|██▍       | 305/1225 [1:30:32<4:15:08, 16.64s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1301 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 27%|██▋       | 331/1225 [1:39:06<4:57:18, 19.95s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1087 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 32%|███▏      | 396/1225 [1:59:03<4:04:37, 17.71s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1028 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 33%|███▎      | 410/1225 [2:03:16<3:41:35, 16.31s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1156 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 35%|███▍      | 424/1225 [2:07:30<3:57:46, 17.81s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1347 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 36%|███▌      | 444/1225 [2:13:26<3:56:19, 18.16s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1102 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 37%|███▋      | 451/1225 [2:15:35<3:53:56, 18.13s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 37%|███▋      | 453/1225 [2:16:12<3:57:14, 18.44s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1633 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 37%|███▋      | 455/1225 [2:16:48<3:51:31, 18.04s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1104 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 37%|███▋      | 458/1225 [2:17:45<3:56:36, 18.51s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1105 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 39%|███▉      | 480/1225 [2:24:07<3:44:09, 18.05s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1078 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 44%|████▎     | 535/1225 [2:39:49<3:13:43, 16.85s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1179 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 44%|████▍     | 538/1225 [2:40:48<3:32:14, 18.54s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (3895 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 45%|████▍     | 549/1225 [2:44:02<3:14:35, 17.27s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1058 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 46%|████▌     | 566/1225 [2:49:03<3:20:24, 18.25s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1325 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 46%|████▋     | 569/1225 [2:50:01<3:27:49, 19.01s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 48%|████▊     | 582/1225 [2:53:44<2:59:46, 16.77s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1116 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 54%|█████▎    | 657/1225 [3:15:38<2:45:53, 17.52s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 55%|█████▌    | 677/1225 [3:21:36<2:33:33, 16.81s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1733 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 56%|█████▌    | 685/1225 [3:23:54<2:29:57, 16.66s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1135 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 58%|█████▊    | 713/1225 [3:31:58<2:26:21, 17.15s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1128 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 59%|█████▉    | 723/1225 [3:34:50<2:27:16, 17.60s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1254 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 59%|█████▉    | 726/1225 [3:35:44<2:29:03, 17.92s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1629 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 60%|██████    | 741/1225 [3:39:59<2:18:19, 17.15s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1079 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 62%|██████▏   | 756/1225 [3:44:24<2:16:28, 17.46s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (5517 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 63%|██████▎   | 767/1225 [3:47:39<2:10:01, 17.03s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1139 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 65%|██████▌   | 798/1225 [3:56:26<2:03:48, 17.40s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1137 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 66%|██████▌   | 806/1225 [3:58:45<1:55:38, 16.56s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (3134 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 71%|███████   | 871/1225 [4:17:33<1:50:02, 18.65s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 78%|███████▊  | 956/1225 [4:42:14<1:21:09, 18.10s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 79%|███████▉  | 968/1225 [4:45:40<1:15:37, 17.65s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (3210 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████▉| 1223/1225 [5:59:56<00:32, 16.44s/it]  Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 1024). Running this sequence through the model will result in indexing errors\n",
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "filename = data_name.replace(\"/\", \"__\")\n",
    "with jsonlines.open(f\"data/gedi-small-bbc-400-omega60-rp10.jsonl\", 'w') as f:\n",
    "    for item in tqdm(dataset, leave=False):\n",
    "        text = item['text'].strip()\n",
    "        # if len(text) >= 1024:\n",
    "        #     text = text[-1024:]\n",
    "        gens = generate_text(text)\n",
    "        # gens = [g['generated_text'][len(text):] for g in gens]\n",
    "        gens = [g[len(text):] for g in gens]\n",
    "        item['generation'] = gens\n",
    "        # item['prediction'] = classifier(gens)\n",
    "        f.write(item)\n",
    "        # print(text)\n",
    "        # print(item['label'])\n",
    "        # print(gens)\n",
    "        # print(item['prediction'])\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m i, item \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(f)):\n\u001b[0;32m     23\u001b[0m   label \u001b[39m=\u001b[39m fix_label[item[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m---> 24\u001b[0m   preds \u001b[39m=\u001b[39m classifier(item[\u001b[39m'\u001b[39m\u001b[39mgeneration\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     25\u001b[0m   \u001b[39m# preds = item['prediction']\u001b[39;00m\n\u001b[0;32m     27\u001b[0m   total[label] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "from collections import defaultdict\n",
    "\n",
    "label2id = {\n",
    "  \"business\": 0,\n",
    "  \"entertainment\": 1,\n",
    "  \"politics\": 2,\n",
    "  \"sport\": 3,\n",
    "  \"tech\": 4\n",
    "}\n",
    "fix_label = {\n",
    "  2: 3,\n",
    "  4: 2,\n",
    "  3: 1,\n",
    "  1: 0,\n",
    "  0: 4\n",
    "}\n",
    "total = defaultdict(lambda: 0)\n",
    "correct = defaultdict(lambda: 0)\n",
    "\n",
    "with jsonlines.open(\"data/gedi-small-bbc-400-omega60.jsonl\") as f:\n",
    "  for i, item in tqdm(enumerate(f)):\n",
    "    label = fix_label[item['label']]\n",
    "    preds = classifier(item['generation'])\n",
    "    # preds = item['prediction']\n",
    "\n",
    "    total[label] += 5\n",
    "    # print(label2id[p['label']], preds)\n",
    "    # if i == 50:\n",
    "    #   break\n",
    "\n",
    "    for p in preds:\n",
    "      if label2id[p['label']] == label:\n",
    "        correct[label] += 1\n",
    "\n",
    "    # break\n",
    "\n",
    "for k in total.keys():\n",
    "    print(k, correct[k] / total[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6692307692307692\n",
      "1 0.6180952380952381\n",
      "2 0.4033057851239669\n",
      "3 0.43854545454545457\n",
      "4 0.5839622641509434\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in sorted(total.keys()):\n",
    "    print(k, correct[k] / total[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {3: 958, 0: 1169, 1: 839, 4: 678, 2: 712})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
