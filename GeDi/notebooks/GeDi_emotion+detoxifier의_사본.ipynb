{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuVEGMIlVqjO"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/salesforce/GeDi/blob/master/GeDi_guided_GPT_2_XL.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRDxg2gNVqjQ"
      },
      "source": [
        "Official implementation of generation with the topic GeDi (pronounced *Jedi*) model based on our paper [GeDi: Generative Discriminator Guided Sequence Generation](https://arxiv.org/abs/2009.06367)\n",
        "\n",
        "Check our github repository for more options (like detoxification and sentiment control) https://github.com/salesforce/GeDi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWgP7huecyHw",
        "outputId": "e3efe0dd-4c31-457a-ad94-3c661007cbd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'GeDi' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/salesforce/GeDi.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeGaZoISeXy2",
        "outputId": "d0f1da22-1ef4-46da-9e57-e1275ba6fa09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\d-project\\detox\\GeDi\n"
          ]
        }
      ],
      "source": [
        "%cd GeDi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXTswSEueE3b"
      },
      "outputs": [],
      "source": [
        "'''Installing transformers v2.8'''\n",
        "\n",
        "!pip install transformers==2.8 datasets jsonlines\n",
        "!pip install -r hf_requirements.txt\n",
        "\n",
        "'''Downloading GeDi topic model checkpoints'''\n",
        "# !wget https://storage.googleapis.com/sfr-gedi-data/gedi_detoxifier.zip\n",
        "# !unzip gedi_detoxifier.zip\n",
        "\n",
        "# !wget https://storage.googleapis.com/sfr-gedi-data/gedi_topic.zip\n",
        "\n",
        "# with zipfile.ZipFile('gedi_topic.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('./')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPaIUrrwt2yI",
        "outputId": "b77b3b85-b9c7-4a32-e32d-f07e07827603"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
            "��ġ ������ �ƴմϴ�.\n",
            "'unzip'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
            "��ġ ������ �ƴմϴ�.\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/sfr-gedi-data/gedi_sentiment.zip\n",
        "!unzip gedi_sentiment.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OgvczCtkdZM0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "import jsonlines\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "from modeling_gpt2 import GPT2LMHeadModel\n",
        "\n",
        "from transformers import (\n",
        "    GPT2Config,\n",
        "    GPT2Tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN_9amB5NEHA",
        "outputId": "bd02ec7b-eaf2-404d-86c5-c6d11ac0b6da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt2-emotion'...\n",
            "Updating files:  58% (7/12)\n",
            "Updating files:  66% (8/12)\n",
            "Updating files:  75% (9/12)\n",
            "Updating files:  83% (10/12)\n",
            "Updating files:  91% (11/12)\n",
            "Updating files: 100% (12/12)\n",
            "Updating files: 100% (12/12), done.\n",
            "Filtering content: 100% (2/2)\n",
            "Filtering content: 100% (2/2), 486.76 MiB | 10.59 MiB/s\n",
            "Filtering content: 100% (2/2), 486.76 MiB | 10.04 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/heegyu/gpt2-emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CGwDhVA2dHoh"
      },
      "outputs": [],
      "source": [
        "mode = \"detoxifier\"\n",
        "code_desired = \"dirty\"\n",
        "code_undesired = \"clean\"\n",
        "model_type = 'gpt2'\n",
        "gen_type = \"gedi\"\n",
        "# gen_model_name_or_path = \"gpt2\"\n",
        "gen_model_name_or_path = \"./gpt2-emotion\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MODEL_CLASSES = {\"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),}\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[\"gpt2\"]\n",
        "tokenizer = tokenizer_class.from_pretrained(gen_model_name_or_path, do_lower_case=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpkvUIdFVqjS"
      },
      "source": [
        "The next step needs to download and convert the GPT2-XL model. \n",
        "\n",
        "This takes a while (usually about 3 minutes to download and another 5 or so to convert). \n",
        "\n",
        "The good news is that once the model is loaded, you can quickly sample from many different prompts and topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsDR1DzhVqjT",
        "outputId": "9895c852-d0a9-448e-f2cd-6e43b50b2c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no logit scale initialized for gpt2\n"
          ]
        }
      ],
      "source": [
        "#Loading GPT2-XL model (1.5B param LM) below, this could take a while.\n",
        "#This requires additional CPU memory overhead to load the pretrained weights in a new model\n",
        "#Due to CPU memory constraints on Colab, we're loading the model in half precision (load_in_half_prec=True) \n",
        "#Do to this change, generations may not always exactly match samples in paper, but sometimes do, and seem to be similar in quality\n",
        "#If you run the notebook with enough CPU RAM (most likely 16GB+), you can try setting load_in_half_prec=False   \n",
        "\n",
        "model = model_class.from_pretrained(gen_model_name_or_path)#, load_in_half_prec=True)\n",
        "model = model.to(device)\n",
        "# model = model.float()\n",
        "\n",
        "gedi_model_name_or_path = 'gedi_detoxifier'\n",
        "gedi_model = model_class.from_pretrained(gedi_model_name_or_path).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orrhZkuKwOXE"
      },
      "source": [
        "### Set arguments for generation\n",
        "\n",
        "You can change the max generation length, or play around with hyperparameter settings. \n",
        "\n",
        "The default hyperparameters were used in the topic model for the paper.\n",
        "\n",
        "More aggressive topic steering can be done by increasing `disc_weight` or `filter_p` (`filter_p` should always be less than 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "o1Waq8Gdudv-"
      },
      "outputs": [],
      "source": [
        "#setting arguments for generation\n",
        "#max generation length\n",
        "gen_length = 200\n",
        "#omega from paper, higher disc_weight means more aggressive topic steering\n",
        "disc_weight = 30\n",
        "#1 - rho from paper, should be between 0 and 1 higher filter_p means more aggressive topic steering\n",
        "filter_p = 0.8\n",
        "#tau from paper, preserves tokens that are classified as correct topic\n",
        "target_p = 0.8\n",
        "#hyperparameter that determines class prior, set to uniform by default\n",
        "class_bias = 0\n",
        "\n",
        "if gen_length>1024:\n",
        "  \n",
        "  length = 1024\n",
        "else:\n",
        "  length = gen_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj9UD2wYjPz6"
      },
      "source": [
        "### Specify prompt and topic to GeDi\n",
        "\n",
        "\n",
        "The topic and prompt can be specified as strings with the `secondary_code` and `prompt` variables below.\n",
        "\n",
        "Note that our GeDi topic model has been trained on only four topics:  `world`, `sports`, `business` and `science` so it performs best on steering generation from GPT-2 towards these topics. However, it also shows some promising zero-shot results on new topics for eg. `education`, `food`, `fire`, `space`, `cars`, `climate`.\n",
        "\n",
        "Generic short prompts tend to work the best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZjDKr8ZAgMN8"
      },
      "outputs": [],
      "source": [
        "#Specify what topic you want to generate on using the secondary_code variable\n",
        "\n",
        "secondary_code = 'climate'\n",
        "bpe_tokens = tokenizer.encode(secondary_code)\n",
        "if len(bpe_tokens) > 1:\n",
        "  print(\"Warning! number of bpe tokens for \" + code + \" is greater than 1, model isn't trained for this, generation is less likely to match the topic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5LyimIvdwEB",
        "outputId": "efaca123-20fe-44d5-a185-3bfe3f3c8e2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ipsays i doubt that i should feel ashamed of the god today because then it would just be one more sarcastic comment about me at a time when alex',\n",
              " ' be my comfort in this troubled world m thats why i love church huge thanks this is being taken away all the people would feel ignored volunteered etc because its against',\n",
              " ' fuck those sins i feel so devastated xoxxd im not sure what has happened but pictured below is the picture that is of me no longer with her',\n",
              " ' shit im feeling stressed driving and i wanna see my mails or read old posts next evening on christmas eve dr ryan home saving man how cool be',\n",
              " ' death of christ and i ever feeling unimportant painful repetitious look down to waste time thinking and just today with a freshemptore fuck that is really his']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def generate_text(prompt, use_gedi=True):\n",
        "  text_ids = tokenizer.encode(prompt)\n",
        "  encoded_prompts=torch.LongTensor(text_ids).unsqueeze(0).to(device)\n",
        "\n",
        "  # multi_code = tokenizer.encode(secondary_code)\n",
        "  attr_class = 1\n",
        "\n",
        "  generated_sequence = model.generate(\n",
        "    input_ids=encoded_prompts,\n",
        "    pad_lens=None,\n",
        "    max_length=encoded_prompts.shape[1] + 32,\n",
        "    min_length=encoded_prompts.shape[1] + 32,\n",
        "    top_k=None,\n",
        "    top_p=1.0,\n",
        "    repetition_penalty= 1.2,\n",
        "    rep_penalty_scale= 10,\n",
        "    eos_token_ids = tokenizer.eos_token_id,\n",
        "    pad_token_id = 0,\n",
        "    do_sample= True,\n",
        "    penalize_cond= True,\n",
        "    gedi_model= gedi_model if use_gedi else None,\n",
        "    tokenizer= tokenizer,\n",
        "    disc_weight= disc_weight,\n",
        "    filter_p = filter_p,\n",
        "    target_p = target_p,\n",
        "    class_bias = class_bias,\n",
        "    attr_class = attr_class,\n",
        "    code_0 = code_desired,\n",
        "    code_1 = code_undesired,\n",
        "    multi_code=None,\n",
        "    num_return_sequences=5\n",
        "    )\n",
        "\n",
        "  texts = [tokenizer.decode(output, skip_special_tokens=True)[len(prompt):] for output in generated_sequence.tolist()[0]]\n",
        "  return texts\n",
        "\n",
        "prompt = \"sadness holy\"\n",
        "generate_text(prompt, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "SV-KYZI0UfFt",
        "outputId": "10305552-c32b-4642-eaa5-0d89d3c32707"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "filename = \"emotion\" #data_name.replace(\"/\", \"__\")\n",
        "num_iters = 200\n",
        "prompts = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
        "\n",
        "with jsonlines.open(f\"data/gedi-{filename}+detox.jsonl\", 'w') as f:\n",
        "  for p in prompts:\n",
        "    for i in tqdm(range(num_iters)):\n",
        "        gens = generate_text(f\"{p} \", True)\n",
        "        item = {\n",
        "            'text': p,\n",
        "            'generation': gens\n",
        "        }\n",
        "        # item['generation'] = gens\n",
        "        f.write(item)\n",
        "        # print(item)\n",
        "        # break\n",
        "        # print(text)\n",
        "        # print(item['label'])\n",
        "        # print(gens)\n",
        "        # print(item['prediction'])\n",
        "        # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OLDkxFyPGid",
        "outputId": "89de38ce-d7df-4ef8-f95d-f006c8f208a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"Unknown task text-classification, available tasks are ['feature-extraction', 'sentiment-analysis', 'ner', 'question-answering', 'fill-mask', 'summarization', 'translation_en_to_fr', 'translation_en_to_de', 'translation_en_to_ro']\"",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m \u001b[39m# device='cuda:0'\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# classifier = pipeline(\"text-classification\", model=\"Aron/distilbert-base-uncased-finetuned-emotion\")\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m classifier \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mtext-classification\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\gedi\\lib\\site-packages\\transformers\\pipelines.py:1560\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, framework, **kwargs)\u001b[0m\n\u001b[0;32m   1558\u001b[0m \u001b[39m# Retrieve the task\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m task \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m SUPPORTED_TASKS:\n\u001b[1;32m-> 1560\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown task \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, available tasks are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(task, \u001b[39mlist\u001b[39m(SUPPORTED_TASKS\u001b[39m.\u001b[39mkeys())))\n\u001b[0;32m   1562\u001b[0m framework \u001b[39m=\u001b[39m framework \u001b[39mor\u001b[39;00m get_framework(model)\n\u001b[0;32m   1564\u001b[0m targeted_task \u001b[39m=\u001b[39m SUPPORTED_TASKS[task]\n",
            "\u001b[1;31mKeyError\u001b[0m: \"Unknown task text-classification, available tasks are ['feature-extraction', 'sentiment-analysis', 'ner', 'question-answering', 'fill-mask', 'summarization', 'translation_en_to_fr', 'translation_en_to_de', 'translation_en_to_ro']\""
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# device='cuda:0'\n",
        "# classifier = pipeline(\"text-classification\", model=\"Aron/distilbert-base-uncased-finetuned-emotion\")\n",
        "classifier = pipeline(\"text-classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178,
          "referenced_widgets": [
            "fd779d14164441bca1611b06ce5ba7d4",
            "8d5c764054a04cd8b08a127d0fb3e6e7",
            "ad5b22b47b45455890dbcde979b0f473",
            "bf76cbb5ad7d4a0d8fd86ae73db4b0b8",
            "f8bb7f07fbdd4e92aabceb3b9dc6c924",
            "498e1d6f25fc492fb94c9da0fd0b871f",
            "a052e619628e409ebe4377257c139cd6",
            "5459fb27eabd4d06a810c200775d3da5",
            "8aaa4dece4fc4ae3841b99f6a97d0607",
            "7d730ee58d3644c69bddff867873b404",
            "75349737272d4561ac2cee2f3be5bbd9"
          ]
        },
        "id": "SW0ILaPE7NlL",
        "outputId": "d7bbc5a5-6da1-4c45-b452-f8ddcc51f097"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'label'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mwith\u001b[39;00m jsonlines\u001b[39m.\u001b[39mopen(\u001b[39m\"\u001b[39m\u001b[39mdata/gedi-emotion+detox.jsonl\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     22\u001b[0m   \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m tqdm(f):\n\u001b[1;32m---> 23\u001b[0m     label \u001b[39m=\u001b[39m fix_label[item[\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n\u001b[0;32m     24\u001b[0m     preds \u001b[39m=\u001b[39m item[\u001b[39m'\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     26\u001b[0m     total[label] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'label'"
          ]
        }
      ],
      "source": [
        "import jsonlines\n",
        "from collections import defaultdict\n",
        "\n",
        "label2id = {\n",
        "  \"business\": 0,\n",
        "  \"entertainment\": 1,\n",
        "  \"politics\": 2,\n",
        "  \"sport\": 3,\n",
        "  \"tech\": 4\n",
        "}\n",
        "fix_label = {\n",
        "  2: 3,\n",
        "  4: 2,\n",
        "  3: 1,\n",
        "  1: 0,\n",
        "  0: 4\n",
        "}\n",
        "total = defaultdict(lambda: 0)\n",
        "correct = defaultdict(lambda: 0)\n",
        "\n",
        "with jsonlines.open(\"data/gedi-emotion+detox.jsonl\") as f:\n",
        "  for item in tqdm(f):\n",
        "    label = fix_label[item['label']]\n",
        "    preds = item['prediction']\n",
        "\n",
        "    total[label] += 5\n",
        "\n",
        "    for p in preds:\n",
        "      if p == label:\n",
        "        correct[label] += 1\n",
        "\n",
        "for k in total.keys():\n",
        "    print(k, correct[k] / total[k])\n",
        "\n",
        "print(total, correct, sep='\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "gedi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16 (main, Jan 11 2023, 16:16:36) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "da88572874ee8aab5163304befaa87140d6d4791e914173cd01313ffbc3a091f"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "498e1d6f25fc492fb94c9da0fd0b871f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5459fb27eabd4d06a810c200775d3da5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "75349737272d4561ac2cee2f3be5bbd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d730ee58d3644c69bddff867873b404": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aaa4dece4fc4ae3841b99f6a97d0607": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d5c764054a04cd8b08a127d0fb3e6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_498e1d6f25fc492fb94c9da0fd0b871f",
            "placeholder": "​",
            "style": "IPY_MODEL_a052e619628e409ebe4377257c139cd6",
            "value": ""
          }
        },
        "a052e619628e409ebe4377257c139cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad5b22b47b45455890dbcde979b0f473": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5459fb27eabd4d06a810c200775d3da5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8aaa4dece4fc4ae3841b99f6a97d0607",
            "value": 1
          }
        },
        "bf76cbb5ad7d4a0d8fd86ae73db4b0b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d730ee58d3644c69bddff867873b404",
            "placeholder": "​",
            "style": "IPY_MODEL_75349737272d4561ac2cee2f3be5bbd9",
            "value": " 1225/? [00:00&lt;00:00, 16953.48it/s]"
          }
        },
        "f8bb7f07fbdd4e92aabceb3b9dc6c924": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd779d14164441bca1611b06ce5ba7d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d5c764054a04cd8b08a127d0fb3e6e7",
              "IPY_MODEL_ad5b22b47b45455890dbcde979b0f473",
              "IPY_MODEL_bf76cbb5ad7d4a0d8fd86ae73db4b0b8"
            ],
            "layout": "IPY_MODEL_f8bb7f07fbdd4e92aabceb3b9dc6c924"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
